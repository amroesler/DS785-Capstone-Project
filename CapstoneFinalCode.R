### DS785 Capstone Project
### Anne Roesler
### Code Submission



## Read in libraries ##
library(MASS)
library(readxl)
library(readr)
library(tidyverse)
library(stringr)
library(tidygeocoder)
library(geosphere)
library(tidycensus)
library(sf)
library(gridExtra)
library(psych)
library(corrplot)
library(car)
library(caret)
library(patchwork)
library(DescTools)
library(glmnet)
library(xgboost)
library(randomForest)
library(SHAPforxgboost)



### Read in and process data ###

#Read in Real estate dataset
real_estate <- read_csv("realtor-data.zip.csv.zip")

#Read in CVI dataset
CVI_domain_values <- read_excel("Master CVI Dataset - Oct 2023.xlsx", sheet = 1)
CVI_indicator_values <- read_excel("Master CVI Dataset - Oct 2023.xlsx", sheet = 2)
CVI_native_units <- read_excel("Master CVI Dataset - Oct 2023.xlsx", sheet = 3)

CVI_native_units <- CVI_native_units %>%
  #Separate latitude and longitude
  separate(`Geographic Coordinates`, into = c("Latitude", "Longitude"), sep = ",") %>%
  rename(CensusTract = `FIPS Code`) %>%
  mutate(
    Latitude = as.numeric(str_sub(Latitude, 2)),
    Longitude = as.numeric(Longitude)
  ) %>%
  rowwise() %>%
  #Create new combined walkability and bikability variable
  mutate(Walk_Bikability = mean(c(Walkability, Bikability), na.rm = TRUE)) %>%
  ungroup()

CVI_domain_values <- CVI_domain_values %>%
  rename(CensusTract = `FIPS Code`) #create consistent ID column name

# Define city parameters
cities <- list(
  Milwaukee = list(
    county_census = "Milwaukee County",
    state = "WI",
    state_RE = "Wisconsin",
    cbd = c(-87.9065, 43.0389),
    census_file = "CensusHomeDataMil.csv"
  ),
  Houston = list(
    county_census = "Harris County",
    state = "TX",
    state_RE = "Texas",
    cbd = c(-95.3652, 29.7528),
    census_file = "CensusHomeDataHou.csv"
  ),
  Denver = list(
    county_census = "Denver County",
    state = "CO",
    state_RE = "Colorado",
    cbd = c(-104.9943, 39.7449),
    census_file = "CensusHomeDataDen.csv"
  )
)

#Function to process kaggle real estate data to city-specific data frames

#Get ZIP Code Tabulation Areas (ZCTAs)

#NOTE: Portions of the following script were adapted from suggestions
#generated by Claude AI (Sonnet 4.5) on Sept. 29, 2025

# Retrieve ZIP code spatial data
zctas <- get_acs(geography = "zcta",
                 variables = "B01001_001",
                 year = 2020,
                 geometry = TRUE)

process_RE_data <- function(city_name, city_params, RE_data) {
  #Function to process real estate dataset and covert location ID to
  #comparable census tract ID
  tracts <- get_acs(geography = "tract",
                    state = city_params$state,
                    county = city_params$county_census,
                    variables = "B01001_001",
                    year = 2023,
                    geometry = TRUE)
  
  zips <- zctas %>%
    st_join(tracts, join = st_intersects) %>%
    filter(!is.na(GEOID.y)) %>%
    pull(GEOID.x) %>%
    unique()
  
  real_estate_city <- RE_data %>%
    filter(zip_code %in% zips & state == city_params$state_RE)
  
  real_estate_sf <- real_estate_city %>%
    left_join(zctas %>% select(GEOID), by = c("zip_code" = "GEOID")) %>%
    st_as_sf()
  
  real_estate_tracts <- st_join(real_estate_sf, tracts %>% select(GEOID), join = st_intersects)
  #End of AI adapted script
  
  real_estate_tract_agg <- real_estate_tracts %>%
    st_drop_geometry() %>%
    group_by(CensusTract = str_sub(GEOID, -11)) %>%
    summarise(
      modal_bed = Mode(bed, na.rm = TRUE)[1],
      modal_bath = Mode(bath, na.rm = TRUE)[1],
      median_acres = median(acre_lot, na.rm = TRUE),
      median_sq_ft = median(house_size, na.rm = TRUE),
      n_properties = n()
    )
  
  return(real_estate_tract_agg)
}

# Function to process each city
process_city <- function(city_name, city_params, CVI_nu_data) {
  
  CVI_df <- CVI_nu_data %>% 
    select(CensusTract, Latitude, Longitude, Walk_Bikability) %>%
    mutate(
      distanceCBD_meters = distHaversine(
        cbind(Longitude, Latitude), 
        city_params$cbd
      ),
      distanceCBD_miles = distanceCBD_meters / 1609.34,
    )
  
  # Retrieve additional census data from tidycensus
  census_data2 <- get_acs(
    geography = "tract",
    state = city_params$state,
    county = city_params$county_census,
    variables = c(
      median_income = "B19013_001",
      total_pop = "B01001_001",
      unemployed = "B23025_005",
      labor_force = "B23025_003",
      median_value = "B25077_001",
      median_year_built = "B25035_001"
    ),
    year = 2023,
    geometry = TRUE
  ) %>%
    select(-moe) %>%
    pivot_wider(names_from = variable, values_from = estimate) %>%
    mutate(
      area_sqmi = as.numeric(st_area(geometry)) / 2589988.11, #sq meters to sq mi
      pop_density = total_pop / area_sqmi,
      unemployment_rate = unemployed / labor_force,
      CensusTract = str_sub(GEOID, -11) #Consistent ID name
    )
  
  # Get real estate data frame
  real_estate_data <- process_RE_data(city_name, city_params, real_estate)
  
  # Merge datasets
  final_data <- CVI_df %>%
    inner_join(
      census_data2 %>% select(CensusTract, median_value, median_year_built, median_income, pop_density, unemployment_rate), 
      by = "CensusTract"
    ) %>%
    filter(!is.na(median_value)) %>%
    inner_join(
      real_estate_data %>% select(CensusTract, modal_bed, modal_bath, median_acres, median_sq_ft),
      by = "CensusTract"
    ) %>%
    select(-c("Latitude", "Longitude", "distanceCBD_meters", "geometry"))
  
  return(final_data)
}


#Process all cities
all_city_data <- list()

for (city_name in names(cities)) {
  all_city_data[[city_name]] <- process_city(
    city_name = city_name,
    city_params = cities[[city_name]],
    CVI_nu_data = CVI_native_units
  )
}


# Access individual city results
real_estate_census_Mil <- all_city_data$Milwaukee
real_estate_census_Hou <- all_city_data$Houston
real_estate_census_Den <- all_city_data$Denver

# Create and view boxplots to see distribution of response variable
b1 <- real_estate_census_Hou %>%
  ggplot(aes(y = median_value)) +
  geom_boxplot() +
  labs(title = "Houston Home Values") +
  theme_minimal()

b2 <- real_estate_census_Den %>%
  ggplot(aes(y = median_value)) +
  geom_boxplot() +
  labs(title = "Denver Home Values") +
  theme_minimal()

b3 <- real_estate_census_Mil %>%
  ggplot(aes(y = median_value)) +
  geom_boxplot() +
  labs(title = "Milwaukee Home Values") +
  theme_minimal()

(b1|b2|b3)

# See where there are remaining NA values
summary(real_estate_census_Mil)
summary(real_estate_census_Hou)
summary(real_estate_census_Den)

# Modify city data frames
real_estate_census_Den <- real_estate_census_Den %>%
  #Remove outliers
  filter(median_value < 1500000) %>%
  #median impute NA value
  mutate(median_year_built = ifelse(is.na(median_year_built), median(median_year_built, na.rm = TRUE), median_year_built))
  

real_estate_census_Hou <- real_estate_census_Hou %>%
  #Remove outliers
  filter(median_value < 1250000) %>%
  #median impute NA values
  mutate(median_year_built = ifelse(is.na(median_year_built), median(median_year_built, na.rm = TRUE), median_year_built),
         median_income = ifelse(is.na(median_income), median(median_income, na.rm = TRUE), median_income))

# Add real estate data frame name to parameters list to set up for next step
cities$Milwaukee$RE_df <- real_estate_census_Mil
cities$Houston$RE_df <- real_estate_census_Hou
cities$Denver$RE_df <- real_estate_census_Den



#Generate data frames for modeling with climate data

#identify climate columns of interest from CVI_native_units
climate_col_names <- c("CensusTract", "Days with maximum temperature above 35 C",	"Days with maximum temperature above 40C",	"Mean temperature",	"Urban Heat Island Extreme Heat Days",	"Drought - Annualized Frequency",	"Total Precipitation",	"Riverine Flooding - Annualized Frequency", "Coastal Flooding - Annualized Frequency",	"Sea Level Rise",	"Hurricane - Annualized Frequency",	"Tornado - Annualized Frequency")

#identify climate columns of interest from CVI_domain values
climate_dv_cols <- c("CensusTract", "Overall CVI Score", "Climate Change: All", "Climate Change: Extreme Events")

add_climate_cols <- function(city_name, city_params, CVI_nu_data, CVI_dv_data) {
  
  final_climate_df <- city_params$RE_df %>%
    inner_join(
      CVI_nu_data %>% select(all_of(climate_col_names)), 
      by = "CensusTract"
    ) %>%
    inner_join(
      CVI_dv_data %>% select(all_of(climate_dv_cols)),
      by = "CensusTract"
    )
  
  return(final_climate_df)
}

#Process all cities
all_city_climate_data <- list()

for (city_name in names(cities)) {
  all_city_climate_data[[city_name]] <- add_climate_cols(
    city_name = city_name,
    city_params = cities[[city_name]],
    CVI_nu_data = CVI_native_units,
    CVI_dv_data = CVI_domain_values
  )
}

# Access individual city results
real_estate_climate_Mil <- all_city_climate_data$Milwaukee
real_estate_climate_Hou <- all_city_climate_data$Houston
real_estate_climate_Den <- all_city_climate_data$Denver

# Check summary for columns with NA values
summary(real_estate_climate_Mil)
summary(real_estate_climate_Hou)
summary(real_estate_climate_Den)

# Modify climate data
real_estate_climate_Den <- real_estate_climate_Den %>%
  #Remove columns that don't apply to Denver
  select(-`Sea Level Rise`, -`Coastal Flooding - Annualized Frequency`, -`Hurricane - Annualized Frequency`) %>%
  #Replace NA where 0 should be
  mutate(`Days with maximum temperature above 40C` = replace_na(`Days with maximum temperature above 40C`, 0),
         `Mean temperature` = replace_na(`Mean temperature`, 0),
         `Total Precipitation` = replace_na(`Total Precipitation`, 0))

real_estate_climate_Hou <- real_estate_climate_Hou %>%
  #Replace NA with 0
  mutate(`Sea Level Rise` = replace_na(`Sea Level Rise`, 0))

real_estate_climate_Mil <- real_estate_climate_Mil %>%
  #Remove columns that don't apply to Milwaukee
  select(-`Sea Level Rise`, -`Coastal Flooding - Annualized Frequency`, -`Hurricane - Annualized Frequency`)



### Explore Variable Scaling/Normality ###
#Update data frame, predictor and title based on what you'd like to view

numeric_re_Mil <- real_estate_census_Mil %>% 
  select(where(is.numeric))

describe(numeric_re_Mil)

#Create table with stats on predictors
re_Mil_stats <- describe(numeric_re_Mil) %>%
  select(min, max, mean, sd, median, range, skew) %>%
  mutate(across(everything(), ~round(., 2)))

View(re_Mil_stats)

#Create dataset with processed variables to compare with original
preproc <- preProcess(numeric_re_Mil[, -which(names(numeric_re_Mil) == "median_value")], 
                      method = c("YeoJohnson", "center", "scale"))

re_Mil_processed <- predict(preproc, numeric_re_Mil)

re_Mil_stats_processed <- describe(re_Mil_processed) %>%
  select(min, max, mean, sd, median, range, skew) %>%
  mutate(across(everything(), ~round(., 2)))

#Density plots before and after processing
p1 <- ggplot(real_estate_census_Mil, aes(x = median_income)) +
  geom_density(fill = "lightblue", color = "black") +
  labs(title = "Original Distribution", x = "Median Income") +
  theme_minimal()

p2 <- ggplot(re_Mil_processed, aes(x = median_income)) +
  geom_density(fill = "lightgreen", color = "black") +
  labs(title = paste("Yeo-Johnson Transformed"),
       x = "Transformed Value") +
  theme_minimal()

# Q-Q plots before and after processing
p3 <- ggplot(real_estate_census_Mil, aes(sample = median_income)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Original Q-Q Plot") +
  theme_minimal()

p4 <- ggplot(re_Mil_processed, aes(sample = median_income)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Transformed Q-Q Plot") +
  theme_minimal()

grid.arrange(p1, p2, p3, p4, ncol = 2)

# Stats of climate predictors
re_Mil_climate_stats <- describe(real_estate_climate_Mil) %>%
  select(min, max) %>%
  mutate(across(everything(), ~round(., 2))) %>%
  mutate(n_distinct = sapply(real_estate_climate_Mil, n_distinct))

View(re_Mil_climate_stats)

### Explore Multicollinearity ###

#Correlation plot

numeric_climate_Mil <- real_estate_climate_Mil %>% 
  select(where(is.numeric),
         -`Total Precipitation`)

re_Mil_Cor <- cor(numeric_climate_Mil)
corrplot(re_Mil_Cor, method = "circle", type = "lower", tl.cex = 0.7)

vif_scores <- vif(lm(median_value ~ ., data = numeric_climate_Mil))

# View VIF scores
print(vif_scores)

vif_df <- data.frame(
  variable = names(vif_scores),
  VIF = vif_scores,
  row.names = NULL
)

View(vif_df)
## Set up training and test sets with 80/20 split

set.seed(6279)

# Milwaukee
x.Mil <- real_estate_census_Mil %>% 
  select(-median_value, -CensusTract) %>% 
  as.matrix()
y.Mil <- real_estate_census_Mil$median_value

train_index_Mil <- createDataPartition(y.Mil, p=0.8, list=FALSE)

x.Mil.train <- x.Mil[train_index_Mil, ]
y.Mil.train <- y.Mil[train_index_Mil]

x.Mil.test <- x.Mil[-train_index_Mil, ]
y.Mil.test <- y.Mil[-train_index_Mil]

# Milwauke with Climate
x.Mil.Climate <- real_estate_climate_Mil %>%
  select(-median_value, -CensusTract) %>% 
  as.matrix()
y.Mil.Climate <- real_estate_climate_Mil$median_value

x.Mil.climate.train <- x.Mil.Climate[train_index_Mil, ]
y.Mil.climate.train <- y.Mil.Climate[train_index_Mil]

x.Mil.climate.test <- x.Mil.Climate[-train_index_Mil, ]
y.Mil.climate.test <- y.Mil.Climate[-train_index_Mil]

# Houston
x.Hou <- real_estate_census_Hou %>%
  select(-median_value, -CensusTract) %>% 
  as.matrix()
y.Hou <- real_estate_census_Hou$median_value

train_index_Hou <- createDataPartition(y.Hou, p=0.8, list=FALSE)

x.Hou.train <- x.Hou[train_index_Hou, ]
y.Hou.train <- y.Hou[train_index_Hou]

x.Hou.test <- x.Hou[-train_index_Hou, ]
y.Hou.test <- y.Hou[-train_index_Hou]

# Houston with Climate
x.Hou.Climate <- real_estate_climate_Hou %>%
  select(-median_value, -CensusTract)%>%
  as.matrix()
y.Hou.Climate <- real_estate_climate_Hou$median_value

x.Hou.climate.train <- x.Hou.Climate[train_index_Hou, ]
y.Hou.climate.train <- y.Hou.Climate[train_index_Hou]

x.Hou.climate.test <- x.Hou.Climate[-train_index_Hou, ]
y.Hou.climate.test <- y.Hou.Climate[-train_index_Hou]

# Denver
x.Den <- real_estate_census_Den %>%
  select(-median_value, -CensusTract) %>%
  as.matrix()
y.Den <- real_estate_census_Den$median_value

train_index_Den <- createDataPartition(y.Den, p=0.8, list=FALSE)

x.Den.train <- x.Den[train_index_Den, ]
y.Den.train <- y.Den[train_index_Den]

x.Den.test <- x.Den[-train_index_Den, ]
y.Den.test <- y.Den[-train_index_Den]

# Denver with Climate
x.Den.Climate <- real_estate_climate_Den %>%
  select(-median_value, -CensusTract) %>%
  as.matrix()
y.Den.Climate <- real_estate_climate_Den$median_value

x.Den.climate.train <- x.Den.Climate[train_index_Den, ]
y.Den.climate.train <- y.Den.Climate[train_index_Den]

x.Den.climate.test <- x.Den.Climate[-train_index_Den, ]
y.Den.climate.test <- y.Den.Climate[-train_index_Den]



### Baseline Linear Regression model ###

# 10-fold repeated cross validation
training_lm <- trainControl(method = "repeatedcv", number = 10, repeats = 10)

# Milwaukee
real_estate_lm_Mil <- train(x = x.Mil.train,
                            y = y.Mil.train,
                            method = "lm",
                            preProcess = c("center", "scale", "YeoJohnson"),
                            trControl = training_lm)

Mil_lm_preds <- predict(real_estate_lm_Mil, newdata = x.Mil.test)
print(paste("Mil RMSE:", sqrt(mean((y.Mil.test - Mil_lm_preds)^2)), 
            "Mil R2:", cor(y.Mil.test, Mil_lm_preds)^2))

real_estate_lm_climate_Mil <- train(x = x.Mil.climate.train,
                                    y = y.Mil.climate.train,
                                    method = "lm",
                                    preProcess = c("center", "scale", "YeoJohnson"),
                                    trControl = training_lm)

Mil_climate_lm_preds <- predict(real_estate_lm_climate_Mil, newdata = x.Mil.climate.test)
print(paste("Mil Climate RMSE:", sqrt(mean((y.Mil.climate.test - Mil_climate_lm_preds)^2)), 
            "Mil Climate R2:", cor(y.Mil.climate.test, Mil_climate_lm_preds)^2))

# Houston
real_estate_lm_Hou <- train(x = x.Hou.train,
                            y = y.Hou.train,
                            method = "lm",
                            preProcess = c("center", "scale", "YeoJohnson"),
                            trControl = training_lm)

Hou_lm_preds <- predict(real_estate_lm_Hou, newdata = x.Hou.test)
print(paste("Hou RMSE:", sqrt(mean((y.Hou.test - Hou_lm_preds)^2)),
            "Hou R2:", cor(y.Hou.test, Hou_lm_preds)^2))

real_estate_lm_climate_Hou <- train(x = x.Hou.climate.train,
                                    y = y.Hou.climate.train,
                                    method = "lm",
                                    preProcess = c("center", "scale", "YeoJohnson"),
                                    trControl = training_lm)

Hou_climate_lm_preds <- predict(real_estate_lm_climate_Hou, newdata = x.Hou.climate.test)
print(paste("Hou Climate RMSE:", sqrt(mean((y.Hou.climate.test - Hou_climate_lm_preds)^2)), 
            "Hou Climate R2:", cor(y.Hou.climate.test, Hou_climate_lm_preds)^2))

# Denver
real_estate_lm_Den <- train(x = x.Den.train,
                            y = y.Den.train,
                            method = "lm",
                            preProcess = c("center", "scale", "YeoJohnson"),
                            trControl = training_lm)

Den_lm_preds <- predict(real_estate_lm_Den, newdata = x.Den.test)
print(paste("Den RMSE:", sqrt(mean((y.Den.test - Den_lm_preds)^2)),
            "Den R2:", cor(y.Den.test, Den_lm_preds)^2))

real_estate_lm_climate_Den <- train(x = x.Den.climate.train,
                                    y = y.Den.climate.train,
                                    method = "lm",
                                    preProcess = c("center", "scale", "YeoJohnson"),
                                    trControl = training_lm)

Den_climate_lm_preds <- predict(real_estate_lm_climate_Den, newdata = x.Den.climate.test)
print(paste("Den Climate RMSE:", sqrt(mean((y.Den.climate.test - Den_climate_lm_preds)^2)), 
            "Den Climate R2:", cor(y.Den.climate.test, Den_climate_lm_preds)^2))



### Initial hyperparameter tuning to refine tune grid for double CV ###

# function to plot parameter comparisons
tuneplot <- function(x, probs = .90) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
    theme_minimal()
}

# XGBoost Initial Tuning
# Function to build table to compare hyperparameters and RMSE
# Rerun to reset for each dataset
tune_cols <- c("nrounds", "max_depth", "eta", "gamma", "colsample_bytree", "min_child_weight", "subsample", "RMSE", "RMSESD", "CV") 
tuning_df <- data.frame(matrix(nrow = 0, ncol = length(tune_cols))) 
colnames(tuning_df) <- tune_cols

build_tuning_df <- function(xgb_model) {
  
  best_results <- merge(xgb_model$bestTune, xgb_model$results)
  
  tuning_df <<- tuning_df %>%
    add_row(nrounds = xgb_model$bestTune$nrounds,
            max_depth = xgb_model$bestTune$max_depth,
            eta = xgb_model$bestTune$eta,
            gamma = xgb_model$bestTune$gamma,
            colsample_bytree = xgb_model$bestTune$colsample_bytree,
            min_child_weight = xgb_model$bestTune$min_child_weight,
            subsample = xgb_model$bestTune$subsample,
            RMSE = min(xgb_model$results$RMSE),
            RMSESD = best_results$RMSESD,
            CV = RMSESD/RMSE) %>%
    arrange(RMSE)
}

#10-fold repeated cross validation
training1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# Iteratively adjust tuning grid to narrow down to best hyperparameters
# expand then narrow tune grid parameters 1 or 2 at a time based on best results
# Use tuning plot and RMSE to determine next parameter values

tune_grid <- expand.grid(
  nrounds = seq(from = 200, to = 1000, by = 200),
  eta = c(.005, .01, .02, .03, .04, .05),
  max_depth = 2,
  gamma = 0,
  colsample_bytree = .9,
  min_child_weight = c(.6, .7, .8, .9, 1),
  subsample = .9
)

# change x and y values to reflect dataset being tuned
xgb_tune <- train(
  x = x.Mil.Climate,
  y = y.Mil.Climate,
  method = "xgbTree",
  verbosity = 0,
  trControl = training1,
  tuneGrid = tune_grid
)

tuneplot(xgb_tune)
xgb_tune$bestTune
min(xgb_tune$results$RMSE)
min(xgb_tune$results$Rsquared)
build_tuning_df(xgb_tune)

# Update tune grids below based on best results of iterative search above
# Uncomment and run when you have final parameter values chosen

tune_grid_Mil1x <- expand.grid(
  nrounds = seq(from = 200, to = 4000, by = 100),
  eta = c(.005, .01, .015, .02, .025, .03, .035),
  max_depth = c(2,3),
  gamma = c(0, .01),
  colsample_bytree = c(.8, .9),
  min_child_weight = 1,
  subsample = c(.8, .9)
)

tune_grid_Mil2x <- expand.grid(
  nrounds = seq(from = 200, to = 1000, by = 100),
  eta = c(.01, .015, .02, .025, .03, .035, .4),
  max_depth = c(2,3),
  gamma = 0,
  colsample_bytree = c(.9, 1),
  min_child_weight = c(.7, .8, .9, 1),
  subsample = c(.9, 1)
)

tune_grid_Hou1x <- expand.grid(
   nrounds = seq(from = 200, to = 3000, by = 100),
   eta = c(.001, .005, .01, .015, .02, .025, .03, .035),
   max_depth = c(2, 3, 4),
   gamma = c(0, .01),
   colsample_bytree = c(.6, .7),
   min_child_weight = 1,
   subsample = c(.4, .5, .6)
)

tune_grid_Hou2x <- expand.grid(
  nrounds = seq(from = 200, to = 8000, by = 100),
  eta = c(.005, .01, .02, .03,.04, .06),
  max_depth = c(2, 3),
  gamma = c(0, .01),
  colsample_bytree = c(.6, .7),
  min_child_weight = 1,
  subsample = c(.4, .5)
)

tune_grid_Den1x <- expand.grid(
  nrounds = seq(from = 200, to = 600, by = 100),
  eta = c(.005, .01, .015, .02, .025, .03, .035, .04),
  max_depth = c(2, 3),
  gamma = c(0, .1),
  colsample_bytree = c(.7, .8),
  min_child_weight = c(1, 2),
  subsample = c(.8, .9, 1)
)

tune_grid_Den2x = expand.grid(
  nrounds = seq(from = 200, to = 1000, by = 100),
  eta = c(.01, .015, .02, .025, .03, .035),
  max_depth = c(5,6),
  gamma = c(0, 0.5, 1),
  colsample_bytree = c(.8, .9),
  min_child_weight = c(1,2),
  subsample = c(.8, .9)
)



# Random forest initial Tuning
# Follow same method as above

# Function to build data frame to compare stats of different tunes
rf_tune_cols <- c("mtry", "RMSE", "RMSESD", "CV") 

tuning_df_rf <- data.frame(matrix(nrow = 0, ncol = length(rf_tune_cols))) 
colnames(tuning_df_rf) <- rf_tune_cols

build_tuning_df_rf <- function(rf_model) {
  
  best_results <- merge(rf_model$bestTune, rf_model$results)
  
  tuning_df_rf <<- tuning_df_rf %>%
    add_row(mtry = rf_model$bestTune$mtry,
            RMSE = min(rf_model$results$RMSE),
            RMSESD = best_results$RMSESD,
            CV = RMSESD/RMSE) %>%
    arrange(RMSE)
}

# Change x and y to reflect dataset being tuned
rf_tune <- train(
  x = x.Den.Climate,
  y = y.Den.Climate,
  method = "rf",
  tuneGrid = expand.grid(mtry = c(15, 16, 17, 18, 19)),
  trControl = training,
  ntree = 500,
  importance = TRUE
)

tuneplot(rf_tune)
rf_tune$bestTune
min(rf_tune$results$RMSE)
build_tuning_df_rf(rf_tune)

# Update tune grids below based on best results of iterative search above
# Uncomment and run when you have final parameter values chosen

tune_grid_Mil1rf <- expand.grid(mtry = c(8, 9, 10, 11))

tune_grid_Mil2rf <- expand.grid(mtry = c(12, 13, 14, 15, 16))

tune_grid_Hou1rf <- expand.grid(mtry = c(5, 6, 7, 8, 9))

tune_grid_Hou2rf <- expand.grid(mtry = c(9, 10, 11, 12, 13, 14))

tune_grid_Den1rf <- expand.grid(mtry = c(5, 6, 7, 8, 9))

tune_grid_Den2rf <- expand.grid(mtry = c(15, 16, 17, 18, 19))



### Double cross validation for model selection ###

#Change dataset based on which city and data being modeled
df_to_use <- real_estate_climate_Den

# Outer shell CV
n = dim(df_to_use)[1]
nfolds = 10
groups = rep(1:nfolds, length=n)
set.seed(6279)
cvgroups = sample(groups, n)

allpredictedCV.out = rep(NA, n)
allbestTypes = rep(NA, nfolds)
allbestPars = vector("list",nfolds)
allbestRMSE = rep(NA, nfolds)

# outer loop
for (jj in 1:nfolds) {
  groupjj <- (cvgroups == jj)
  
  traindata <- df_to_use[!groupjj, ]
  trainx <- traindata %>% 
    select(-median_value, -CensusTract) %>%
    as.matrix()
  trainy <- traindata$median_value
  validdata <- df_to_use[groupjj, ]
  validx <- validdata %>% 
    select(-median_value, -CensusTract) %>%
    as.matrix()
  validy <- validdata$median_value
  
  ### Model fitting process on traindata ###
  training <- trainControl(method = "cv", number = 5)
  
  #Generate lambda sequence for this fold
  temp_glmnet <- glmnet(trainx, trainy, alpha = 0.5)
  lambda_seq_this_fold <- temp_glmnet$lambda
  
  
  fit_enet <- train(x = trainx,
                    y = trainy,
                    method = "glmnet",
                    preProcess = c("center", "scale", "YeoJohnson"),
                    trControl = training,
                    tuneGrid = expand.grid(alpha = seq(0, 1, by = 0.1),
                                           lambda = lambda_seq_this_fold))
  
  fit_rf <- train(x = trainx,
                  y = trainy,
                  method = "rf",
                  tuneGrid = tune_grid_Den2rf, #insert grid that corresponds to df being used
                  trControl = training,
                  ntree = 500,
                  importance = TRUE)
  
  fit_boosted <- train(x = trainx,
                       y = trainy,
                       method = "xgbTree",
                       verbosity = 0,
                       trControl = training,
                       tuneGrid = tune_grid_Den2x) #insert grid that corresponds to df being used
  
  ### Identify selected model ###
  all_best_Types = c("enet", "RF", "Boosted")
  all_best_Pars = list(fit_enet$bestTune, fit_rf$bestTune, fit_boosted$bestTune)
  all_best_Models = list(fit_enet$finalModel,
                         fit_rf$finalModel,
                         fit_boosted$finalModel)
  all_best_RMSE = c(min(fit_enet$results$RMSE),
                    min(fit_rf$results$RMSE),
                    min(fit_boosted$results$RMSE))
  
  ### visual model comparison ###
  
  mEnet = nrow(fit_enet$results)
  mRF = nrow(fit_rf$results)
  mBoost = nrow(fit_boosted$results)
  mmodels = mEnet + mRF + mBoost
  
  plot_data <- data.frame(
    Model_Index = 1:mmodels,
    RMSE = c(fit_enet$results$RMSE,
             fit_rf$results$RMSE,
             fit_boosted$results$RMSE),
    Model_Type = c(rep("Enet", mEnet), 
                   rep("RF", mRF), 
                   rep("Boosted", mBoost)))
  
  # Find minimum positions for each model type
  min_positions <- data.frame(
    xintercept = c(1, 
                   mEnet + 1, 
                   mEnet + mRF + which.min(fit_boosted$results$RMSE)),
    Model_Type = c("Enet", "RF", "Boosted"))
  
  # Find overall minimum
  overall_min_pos <- which.min(plot_data$RMSE)
  
  # Create the plot
  p <- ggplot(plot_data, aes(x = Model_Index, y = RMSE, color = Model_Type)) +
    geom_point(aes(shape = Model_Type), size = 3) +
    geom_vline(data = min_positions, 
               aes(xintercept = xintercept), 
               linewidth = 1, 
               linetype = "solid") +
    geom_vline(xintercept = overall_min_pos, 
               color = "red", 
               linewidth = 1, 
               linetype = "solid") +
    scale_color_brewer(palette = "Set1") +
    scale_shape_manual(values = c(15, 16, 17)) +
    labs(x = "Model Label", 
         y = "RMSE",
         color = "Model Type",
         shape = "Model Type") +
    theme_minimal() +
    theme(legend.position = "right")
  
  print(p)
  
  one_best_Type = all_best_Types[which.min(all_best_RMSE)]
  one_best_Pars = all_best_Pars[which.min(all_best_RMSE)]
  one_best_Model = all_best_Models[[which.min(all_best_RMSE)]]
  one_best_RMSE = min(all_best_RMSE)
  
  allbestTypes[jj] = one_best_Type
  allbestPars[[jj]] = one_best_Pars
  allbestRMSE[jj] = one_best_RMSE
  
  if (one_best_Type == "enet") {
    allpredictedCV.out[groupjj] = predict(fit_enet, newdata = validx)
  } else if (one_best_Type == "RF") {
    allpredictedCV.out[groupjj] = predict(fit_rf, newdata = validx)
  } else if (one_best_Type == "Boosted") {
    allpredictedCV.out[groupjj] = predict(fit_boosted, newdata = validx)
  }
  
}

# print out best model for each loop
for (jj in 1:nfolds) {
  writemodel = paste("The best model at loop", jj, 
                     "is of type", allbestTypes[jj],
                     "with parameter(s)",allbestPars[[jj]],
                     "and RMSE of", allbestRMSE[jj])
  print(writemodel, quote = FALSE)
}

### Assessment ###
# Model 1 for each city = real estate data only
# Model 2 for each city = real estate and climate vulnerability data
# Uncomment for model you are running above double CV for
y = df_to_use$median_value

# RMSE.Mil1 = sqrt(mean((allpredictedCV.out - y)^2)); RMSE.Mil1
# R2.Mil1 = 1 - sum((allpredictedCV.out - y)^2)/sum((y-mean(y))^2); R2.Mil1

# RMSE.Mil2 = sqrt(mean((allpredictedCV.out - y)^2)); RMSE.Mil2
# R2.Mil2 = 1 - sum((allpredictedCV.out - y)^2)/sum((y-mean(y))^2); R2.Mil2

# RMSE.Hou1 = sqrt(mean((allpredictedCV.out - y)^2)); RMSE.Hou1
# R2.Hou1 = 1 - sum((allpredictedCV.out - y)^2)/sum((y-mean(y))^2); R2.Hou1
 
# RMSE.Hou2 = sqrt(mean((allpredictedCV.out - y)^2)); RMSE.Hou2
# R2.Hou2 = 1 - sum((allpredictedCV.out - y)^2)/sum((y-mean(y))^2); R2.Hou2

# RMSE.Den1 = sqrt(mean((allpredictedCV.out - y)^2)); RMSE.Den1
# R2.Den1 = 1 - sum((allpredictedCV.out - y)^2)/sum((y-mean(y))^2); R2.Den1
 
RMSE.Den2 = sqrt(mean((allpredictedCV.out - y)^2)); RMSE.Den2
R2.Den2 = 1 - sum((allpredictedCV.out - y)^2)/sum((y-mean(y))^2); R2.Den2


# Calculate residuals
# residuals_Mil.model1 <- y - allpredictedCV.out
# residuals_Mil.model2 <- y - allpredictedCV.out
# residuals_Hou.model1 <- y - allpredictedCV.out
# residuals_Hou.model2 <- y - allpredictedCV.out
# residuals_Den.model1 <- y - allpredictedCV.out
residuals_Den.model2 <- y - allpredictedCV.out

# Squared errors
# sq_errors_Mil.model1 <- residuals_Mil.model1^2
# sq_errors_Mil.model2 <- residuals_Mil.model2^2
# sq_errors_Hou.model1 <- residuals_Hou.model1^2
# sq_errors_Hou.model2 <- residuals_Hou.model2^2
# sq_errors_Den.model1 <- residuals_Den.model1^2
sq_errors_Den.model2 <- residuals_Den.model2^2



### Further assessment and visualizations ###

# Paired t-test on squared errors
t_test_sq_errors.Mil <- t.test(sq_errors_Mil.model1, sq_errors_Mil.model2, paired = TRUE)
t_test_sq_errors.Hou <- t.test(sq_errors_Hou.model1, sq_errors_Hou.model2, paired = TRUE)
t_test_sq_errors.Den <- t.test(sq_errors_Den.model1, sq_errors_Den.model2, paired = TRUE)

# P-values

p_value.Mil <- t_test_sq_errors.Mil$p.value; print(paste("P-value for Milwaukee climate added improvement:", p_value.Mil))
p_value.Hou <- t_test_sq_errors.Hou$p.value; print(paste("P-value for Houston climate added improvement:", p_value.Hou))
p_value.Den <- t_test_sq_errors.Den$p.value; print(paste("P-value for Denver climate added improvement:", p_value.Den))

#Error plots
current_model_residuals <- residuals_Den.model2 # Change based on which model
ggplot() +
  aes(x = current_model_residuals) +
  geom_histogram(fill="darkolivegreen3") +
  labs(title = "Denver with Climate") + #Change title to reflect model
  theme_minimal()

ggplot() +
  aes(sample = current_model_residuals) +
  geom_qq_line() +
  stat_qq() +
  theme_minimal() +
  labs(title = "Normal Q-Q Plot")

ggplot() +
  aes(x = allpredictedCV.out, y = current_model_residuals) +
  geom_point() +
  theme_minimal() +
  labs(x = "Predicted Values",
       y = "Residuals",
       title = "Residual vs. Fitted")



### Refit model "winner" on full climate dataset for each city ###
### For variable importance and SHAP analysis ###

# Best model Milwaukee = XGBoost
# Best model Houston = XGBoost
# Best model Denver = E-net

final_model_Mil <- train(x = x.Mil.Climate,
                         y = y.Mil.Climate,
                         method = "xgbTree",
                         verbosity = 0,
                         trControl = training,
                         tuneGrid = tune_grid_Mil2x)

final_model_Hou <- train(x = x.Hou.Climate,
                         y = y.Hou.Climate,
                         method = "xgbTree",
                         verbosity = 0,
                         trControl = training,
                         tuneGrid = tune_grid_Hou2x)

temp_glmnet_Den <- glmnet(x.Den.Climate, y.Den.Climate, alpha = 0.5)
lambda_seq_Den <- temp_glmnet_Den$lambda

final_model_Den <- train(x = x.Den.Climate,
                         y = y.Den.Climate,
                         method = "glmnet",
                         trControl = training,
                         tuneGrid = expand.grid(alpha = seq(0, 1, by = 0.1),
                                                lambda = lambda_seq_Den))

# Variable Importance plot
imp_matrix_Mil2 <- xgb.importance(model = final_model_Mil$finalModel)
varImp(final_model_Mil)
import_matrix = xgb.importance(model = final_model_Mil$finalModel)
import_matrix
xgb.plot.importance(import_matrix)
xgb.plot.deepness(model = final_model_Mil$finalModel)

xgb.imp.plot_Mil2 <- xgb.ggplot.importance(importance_matrix = imp_matrix_Mil2, top_n = 20)

# Customize the plot
xgb.imp.plot_Mil2 +
  labs(title = "Variable Importance - Milwaukee with Climate") +
  theme_minimal()

# SHAP plot
#NOTE: Portions of the following script were adapted from suggestions
#generated by Claude AI (Sonnet 4.5) on Nov. 20, 2025
dtrain.Mil2 <- xgb.DMatrix(data = x.Mil.Climate, label = y.Mil.Climate)

shap_values <- shap.values(xgb_model = final_model_Mil$finalModel, X_train = x.Mil.Climate)

CV_features <- c("Overall CVI Score", "Tornado - Annualized Frequency", "Climate Change: Extreme Events")

climate_indices <- which(colnames(x.Mil.Climate) %in% CV_features)

CV_shap <- shap_values$shap_score[, ..climate_indices, drop = FALSE]

CV_impact <- rowSums(CV_shap)

base_value <- shap_values$BIAS0

CV_shap_df <- data.frame(
  CensusTract = real_estate_climate_Mil$CensusTract,
  actual_value = real_estate_climate_Mil$median_value,
  base_prediction = base_value,
  climate_impact = CV_impact) %>%
  mutate(climate_impact_pct = (climate_impact/base_value) * 100,
         vulnerability_rank = rank(climate_impact),
         abs_value_loss = abs(climate_impact)) %>%
  arrange(climate_impact)

CV_shap_slice <- CV_shap_df %>%
  slice_head(n = 3) %>%
  select(CensusTract, climate_impact)

print(CV_shap_slice)

ggplot(CV_shap_df, aes(x = climate_impact)) +
  geom_histogram() +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Distribution of Climate Impact on Property Values",
       subtitle = "Negative = Climate factors reduce value",
       x = "Total Climate Impact",
       y = "Number of Census Tracts") +
  theme_minimal()

shap_long <- shap.prep(shap_contrib = shap_values$shap_score,
                       X_train = as.data.frame(x.Mil.Climate),
                       top_n = 15)

# Create the plot
shap.plot.summary(shap_long)
# End of AI adapted script

